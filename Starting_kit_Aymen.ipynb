{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "<table style=\"width:100%; background-color:transparent;\">\n",
    "  <tr style=\"background-color:transparent;\">\n",
    "    <td style=\"background-color:transparent;\">[<img src=\"http://project.inria.fr/saclaycds/files/2017/02/logoUPSayPlusCDS_990.png\" width=\"70%\">](http://www.datascience-paris-saclay.fr)</td>\n",
    "  </tr>\n",
    "</table> \n",
    "</div>\n",
    "\n",
    "\n",
    "# RAMP on predecting Hard drive failure\n",
    "\n",
    "_Salma Jeridi, Aymen Dabghi, Aymen MTIBAA, Ahmed Akakzia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a hard drive failure ? \n",
    "\n",
    "Storage systems are growing larger quickly with the rapid development of information technology. Although hard drives are reliable in general, they are believed to be the most commonly replaced hardware components. It is reported that 78% of all hardware replacements were for hard drives in the data centers of Microsoft. A hard drive failure has consequences ranging from temporary system unavailability to complete data loss.\n",
    "\n",
    "A hard disk drive failure occurs when a hard disk drive malfunctions and the stored information cannot be accessed with a properly configured computer. This may occur in the course of normal operation, or due to an external factor such as exposure to fire or water or high magnetic fields, or suffering a sharp impact or environmental contamination, which can lead to a head crash.\n",
    "\n",
    "Hard drives may also be rendered inoperable through data corruption, disruption or destruction of the hard drive's master boot record, or through malware deliberately destroying the disk's contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we prevent hard drive failure ? \n",
    "\n",
    "\n",
    "Given the importance of users’ data, operators insist on high levels of reliability, and typically either store multiple copies of data throughout the system (increasing storage costs), or store data using some kind of erasurecode (which consumes resources during reconstruction). On top of this, disk failure prediction is used to further increase reliability via automatic backing up of data on at-risk disks. \n",
    "\n",
    "A lot of researchers focus on designing erasure codes to improve storage system reliability. This is a typical reactive fault-tolerant technique which is used to reconstruct data when drive failure occurs. By contrast, predicting drive failures before they actually occur can inform us to take actions in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the main challenges of this prediction task ? \n",
    "\n",
    "\n",
    "* First of all, the high reliability of a hard drive implies that failures have to be considered as rare events. As a result, a real operational Dataset will contain an extremely unbalanced ratio between healthy and failure samples.\n",
    "\n",
    "<br>\n",
    "* Next, it is difficult to obtain sufficient failure occurrences. Indeed, hard drive manufacturers themselves provide data on the failure characteristics of their disks but it has been shown to be inaccurate and often based on extrapolating from the behaviour of a small population in an accelerated life test environment. For this reason, it is important to work with ** operational data collected over a large period of time ** to ensure that it contains enough samples of hard drive failures.\n",
    "\n",
    "<br>\n",
    "* Another challenge is that the Self-Monitoring, Analysis and Reporting Technology (SMART) used to monitor hard drives is not completely standardized. Indeed, the measured set of attributes and the details of SMART implementation are different for every hard drive manufacturer. From a machine learning point of view, there is no guarantee that a learning model trained to predict the failures of a specific hard drive model will be able to accurately predict the failures of another hard drive model. For this reason, in order to draw conclusions on hard drive failure prediction in general, it is important to ensure that the proposed predictive models are estimated from a variety of hard drive models from different manufacturers and also tested on a variety of hard drive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "\n",
    "This RAMP challenge proposes to use different machine learning approach to tackle the problem of predicting Hard drive failure.  The goal of the classification is to determine a function that given a sample x associates a label y ∈ {0, 1} that matches the failure label of the dataset.\n",
    "\n",
    "The predictor needs to be robust to the differences in SMART parameters, and needs to deal with an extremely unbalanced ratio between healthy and failure samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric\n",
    "\n",
    "\n",
    "Many existing solutions for predicting Hard drive failure adopt the detection and false alarm rates as an evaluation metric. However, This is a relevant choice for balanced datasets but, as operational datasets are extremely unbalanced in favour of healthy samples, even a low false alarm rate in the range of 1% could translate into poor performances. Therefore, we rather report **precision and recall metrics.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "In order to overcome these issues related to the dataset and to provide reproducible results, we consider a large, operational and publicly available dataset from Backblaze, a recent operational dataset from over 47,000 drives, exhibiting hard drive heterogeneity with 81 models from 5 manufacturers, an extremely unbalanced ratio of 5000:1 between healthy and failure samples.\n",
    "\n",
    "It gathers daily measurements of SMART parameters of each operational hard drive disk of this company data centre. Updates to the dataset are provided quarterly. The fields of the daily reports are composed as follows: the date of the report, the serial\n",
    "number of the drive, the model of the drive, the capacity of the drive in bytes, a failure label that is at 0 as long as the drive is healthy and that is set to 1 when the drive fails and finally the SMART parameters.\n",
    "\n",
    "The SMART parameters include, for example, counts of read errors, write faults, the temperature of the drive and its reallocated sectors count. However, it should be noted that most drives do not report every parameter resulting in many blank fields. The reason why is that hard drive disk manufacturers are free to decide how to implement SMART parameters. For the same reason, there is no guarantee that SMART parameters from two different models or manufacturers have the same meaning since the details of their implementation are not disclosed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [internal notes] Pre-processing ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally used outlier filtering techniques such as Extreme Value Filtering are inadequate for SMART values of an operational set as it is difficult to distinguish between exceptional values caused by a measurement artefact or by an anomalous behaviour that may lead to a hard drive failure. A classical approach is to limit filtering to obvious errors. In our case, this corresponds to physically impossible values such as power-on hours exceeding 30 years. We filter on two SMART parameters, SMART 9, power-on hours, and SMART 194, temperature. In turned out that this filtering is negligible and does not noticeably impact the dataset with only 5 drives concerned, matching the observation in [2] that less than 0.1% of the hard drives are concerned. Similarly to what is done in [8], we define a time window for failure in the following manner: after a drive fails, we relabel a posteriori the N previous samples from this particular drive as failures where N is the length of the time window in days. In other words, with our classification model we try to answer the question ”Is the hard drive going to fail in the next N days?”. This length will be optimized as a hyper parameter of the model to determine its optimal value if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
